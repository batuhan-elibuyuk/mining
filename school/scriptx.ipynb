{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model Year': '2000', 'Exterior color': 'Lunar Mist Metallic', 'Interior color': 'Gray', 'Drivetrain': 'Front-wheel Drive', 'MPG': '23â€“32', 'Fuel type': 'Gasoline', 'Transmission': 'Automatic', 'Engine': '2.2L I-4 DOHC, regular unleaded, engine with 133HP', 'VIN': '4T1BG22K9YU693763', 'Stock #': '32551', 'Mileage': '134,674 mi.', 'Accidents or damage': 'None reported', '1-owner vehicle': 'No', 'Personal use only': 'Yes', 'Last Listed Price': '$7,000'}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import json\n",
    "\n",
    "\n",
    "# Set up ChromeDriver options\n",
    "options = Options()\n",
    "options.headless = True  # Runs Chrome in headless mode.\n",
    "options.add_argument(r\"C:\\Users\\batuh\\AppData\\Local\\Google\\Chrome\\User Data\")\n",
    "service = Service(executable_path=r'C:\\Users\\batuh\\Desktop\\chromedriver.exe')  # Update with the path to your ChromeDriver\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_car_details(url):\n",
    "    driver.get(url)\n",
    "    \n",
    "    car_data = {}\n",
    "\n",
    "    # Extracting model year from the listing title\n",
    "    try:\n",
    "        title_element = driver.find_element(By.CSS_SELECTOR, \"h1.listing-title\")\n",
    "        title_text = title_element.text.strip()\n",
    "        # Assuming the year is always the first four characters\n",
    "        model_year = title_text.split()[0]\n",
    "        car_data[\"Model Year\"] = model_year\n",
    "    except NoSuchElementException:\n",
    "        car_data[\"Model Year\"] = \"Year not found\"\n",
    "\n",
    "    # Extracting details from <dl> elements\n",
    "    elements = driver.find_elements(By.CSS_SELECTOR, \"dl.fancy-description-list dt\")\n",
    "    for element in elements:\n",
    "        key = element.text.strip()\n",
    "        value_element = element.find_element(By.XPATH, \"./following-sibling::dd[1]\")\n",
    "        value = value_element.text.strip()\n",
    "        car_data[key] = value\n",
    "\n",
    "    # Extracting the most recent list price from the price history table\n",
    "    try:\n",
    "        price_entries = driver.find_elements(By.CSS_SELECTOR, \"div.price-history table tbody tr\")\n",
    "        last_price_element = price_entries[-1].find_element(By.CSS_SELECTOR, \"td.list-price\")\n",
    "        last_price = last_price_element.text.strip()\n",
    "        car_data[\"Last Listed Price\"] = last_price\n",
    "    except (NoSuchElementException, IndexError):\n",
    "        car_data[\"Last Listed Price\"] = \"Last price not found\"\n",
    "\n",
    "    return car_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def scrape_cars(start_url, total_pages=1, batch_size=20):\n",
    "#     cars = []\n",
    "#     original_url = start_url  # Store the original start URL\n",
    "    \n",
    "#     # Collect all car URLs\n",
    "#     all_car_urls = []\n",
    "#     for page in range(total_pages):\n",
    "#         print(f\"Collecting car URLs from page {page + 1}\")\n",
    "#         driver.get(f\"{start_url}&page={page + 1}\")\n",
    "#         time.sleep(3)  # Allow some time for the page to load\n",
    "        \n",
    "#         # Collect vehicle card URLs\n",
    "#         vehicle_card_links = driver.find_elements(By.CSS_SELECTOR, \"a.vehicle-card-visited-tracking-link\")\n",
    "#         car_urls = [link.get_attribute('href') for link in vehicle_card_links]\n",
    "#         all_car_urls.extend(car_urls)\n",
    "    \n",
    "#     # Process car URLs in batches\n",
    "#     for i in range(0, len(all_car_urls), batch_size):\n",
    "#         batch_urls = all_car_urls[i:i+batch_size]\n",
    "#         print(f\"Processing batch {i//batch_size + 1}/{len(all_car_urls)//batch_size}\")\n",
    "#         batch_cars = []\n",
    "#         for car_url in batch_urls:\n",
    "#             batch_cars.append(get_car_details(car_url))\n",
    "#         cars.extend(batch_cars)\n",
    "    \n",
    "#     return cars\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# URL to start with\n",
    "start_url = \"https://www.cars.com/shopping/results/?dealer_id=&keyword=&list_price_max=&list_price_min=&makes[]=toyota&maximum_distance=all&mileage_max=&monthly_payment=&page_size=100&sort=best_match_desc&stock_type=used&year_max=&year_min=&zip=\"\n",
    "\n",
    "url=\"https://www.cars.com/vehicledetail/59d0d05b-e54f-449d-8a7c-e52745bdf95d/?attribution_type=isa\"\n",
    "\n",
    "car_list=get_car_details(url)\n",
    "\n",
    "print(car_list)\n",
    "\n",
    "# df = pd.DataFrame(car_list)\n",
    "# df.to_csv('car_data.csv', index=False)\n",
    "# print(\"Data saved to 'car_data.csv'.\")\n",
    "\n",
    "# car_details=get_car_details(url)\n",
    "\n",
    "# Scrape the cars\n",
    "# car_list = scrape_cars(start_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car_list = scrape_cars(start_url) \n",
    "\n",
    "# # Closing the driver\n",
    "# driver.quit()\n",
    "\n",
    "# # Creating a DataFrame and saving to CSV\n",
    "# df = pd.DataFrame(car_list)\n",
    "# df.to_csv('car_data.csv', index=False)\n",
    "# print(\"Data saved to 'car_data.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Data saved to 'car_data.json'.\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scrape_cars(start_url):\n",
    "\n",
    "    cars = []\n",
    "\n",
    "    # Navigate to the URL\n",
    "    driver.get(start_url)\n",
    "    time.sleep(3)  # Allow time for the page to load\n",
    "\n",
    "    # Collect vehicle card URLs\n",
    "    vehicle_card_links = driver.find_elements(By.CSS_SELECTOR, \"a.vehicle-card-visited-tracking-link\")\n",
    "    all_car_urls = [link.get_attribute('href') for link in vehicle_card_links]\n",
    "\n",
    "    # Process each car URL to extract details\n",
    "    for car_url in all_car_urls:\n",
    "        car_details = get_car_details(car_url)\n",
    "        cars.append(car_details)\n",
    "    \n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "    # Save the data to a JSON file\n",
    "    with open('car_data2.json', 'w') as f:\n",
    "        json.dump(cars, f, indent=4)\n",
    "\n",
    "    return \"Data saved to 'car_data.json'.\"\n",
    "\n",
    "scrape_cars(start_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
